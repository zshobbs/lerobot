import sys
import shutil
from pathlib import Path

# Add src to sys.path to ensure we can import lerobot modules
sys.path.append("./src")

from lerobot.datasets.lerobot_dataset import LeRobotDataset

def merge_datasets():
    # Configuration
    source_paths = [
        Path("/Users/zeke/Movies/prop"),
        Path("/Users/zeke/Movies/prop3")
    ]
    output_path = Path("/Users/zeke/Movies/merged_prop")
    repo_id = "merged_prop"

    # specific keys that are generated by the dataset class and should not be copied directly
    ignore_features = ["index", "episode_index", "frame_index", "timestamp", "task_index"]

    if output_path.exists():
        print(f"Error: Output path {output_path} already exists.")
        print("Please delete it or choose a different output name.")
        return

    # 1. Validation & Setup
    print(f"Reading configuration from {source_paths[0]}...")
    try:
        ds1 = LeRobotDataset(root=source_paths[0], repo_id="prop")
    except Exception as e:
        print(f"Failed to load first dataset: {e}")
        return

    fps = ds1.fps
    robot_type = ds1.meta.robot_type
    
    # Filter features for creation (remove internal metadata keys if they exist in the feature list)
    # The dataset class usually handles adding these, but we ensure we pass a clean dict.
    creation_features = {}
    for k, v in ds1.features.items():
        if k not in ignore_features:
            creation_features[k] = v

    print(f"Creating new dataset at: {output_path}")
    print(f" - FPS: {fps}")
    print(f" - Robot Type: {robot_type}")
    print(f" - Features: {list(creation_features.keys())}")

    ds_out = LeRobotDataset.create(
        repo_id=repo_id,
        root=output_path,
        fps=fps,
        features=creation_features,
        robot_type=robot_type,
        use_videos=True
    )

    total_episodes_processed = 0

    # 2. Merge Loop
    for p in source_paths:
        print(f"\nProcessing source dataset: {p}")
        
        try:
            ds_in = LeRobotDataset(root=p, repo_id=p.name)
        except Exception as e:
            print(f"Error loading {p}: {e}")
            continue

        if ds_in.fps != fps:
            print(f"CRITICAL WARNING: FPS mismatch for {p} ({ds_in.fps} vs {fps}). Skipping.")
            continue

        num_episodes = ds_in.num_episodes
        print(f"Found {num_episodes} episodes.")

        for ep_idx in range(num_episodes):
            print(f"  > Merging episode {ep_idx + 1}/{num_episodes} (Global: {total_episodes_processed + 1})...", end="\r")
            
            # We access the internal logic to get exact frame ranges for efficiency
            # but we use the public API __getitem__ to load data safely.
            ep_meta = ds_in.meta.episodes[ep_idx]
            start_idx = ep_meta["dataset_from_index"]
            end_idx = ep_meta["dataset_to_index"]

            for i in range(start_idx, end_idx):
                # Load frame (this decodes video if necessary)
                frame = ds_in[i]

                # Remove metadata that will be regenerated
                for k in ignore_features:
                    if k in frame:
                        del frame[k]
                
                # Fix image shapes: CHW (torch) -> HWC (expected by add_frame/validate_frame)
                # The dataset loader returns PyTorch tensors (C, H, W), but the schema expects (H, W, C).
                for k in ds_out.meta.camera_keys:
                    if k in frame:
                        val = frame[k]
                        # Check if it is a tensor and looks like CHW (3 channels first)
                        if hasattr(val, "shape") and len(val.shape) == 3 and val.shape[0] in [1, 3]:
                             # Permute to HWC
                             if hasattr(val, "permute"):
                                 frame[k] = val.permute(1, 2, 0)
                             elif hasattr(val, "transpose"): # numpy
                                 frame[k] = val.transpose(1, 2, 0)

                # Add to the new dataset buffer
                ds_out.add_frame(frame)

            # Save the buffered episode to disk (encodes video)
            ds_out.save_episode()
            total_episodes_processed += 1
        
        print(f"\nFinished {p.name}")

    # 3. Finalize
    print("\nFinalizing dataset (writing stats and metadata)...")
    ds_out.finalize()
    print(f"Success! Merged dataset saved to {output_path}")

if __name__ == "__main__":
    merge_datasets()
